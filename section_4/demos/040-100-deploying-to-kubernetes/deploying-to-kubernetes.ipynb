{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment on Kubernetes\n",
    "In this demo we are going to look at different ways to run Model Deployments on Kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Having a Look around\n",
    "In the `manifests` directory.\n",
    "\n",
    "```bash\n",
    "manifests\n",
    "├── deployment.yaml\n",
    "├── hpa.yaml\n",
    "├── service.yaml\n",
    "```\n",
    "\n",
    "Deploy it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Horizontal Pod Autoscaler\n",
    "HPA plays a huge role in being able to handle scaling your applications up and down.\n",
    "\n",
    "This is especially useful for ML/AI workloads.\n",
    "\n",
    "```yaml\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: ml-model-hpa\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: ml-model-deployment\n",
    "  minReplicas: 2\n",
    "  maxReplicas: 10\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70\n",
    "```\n",
    "\n",
    "`scaleTargetRef` defines what we are going to target to scale.\n",
    "\n",
    "`minReplicas` and `maxRelicas` define the minimum and maximum number of pods to be running high or light load.\n",
    "\n",
    "`metrics` is where we define the target resource to watch in order to trigger scaling events. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Affinity\n",
    "\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: ml-model-deployment\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: ml-model\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: ml-model\n",
    "    spec:\n",
    "      spec:\n",
    "      affinity:\n",
    "        nodeAffinity:\n",
    "          requiredDuringSchedulingIgnoredDuringExecution:\n",
    "            nodeSelectorTerms:\n",
    "            - matchExpressions:\n",
    "              - key: kubernetes.io/hostname\n",
    "                operator: In\n",
    "                values:\n",
    "                - \"node02\"\n",
    "      containers:\n",
    "      - name: ml-model-container\n",
    "        image: wbassler/mobilenetv3lg-flask:v1.0\n",
    "        imagePullPolicy: Always\n",
    "        resources:\n",
    "          requests:\n",
    "            cpu: \"200m\"\n",
    "            memory: \"250Mi\"\n",
    "          limits:\n",
    "            cpu: \"200m\"\n",
    "            memory: \"250Mi\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Selector\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: ml-model-deployment\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: ml-model\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: ml-model\n",
    "    spec:\n",
    "      nodeSelector:\n",
    "        kubernetes.io/hostname: \"node02\"\n",
    "      containers:\n",
    "      - name: ml-model-container\n",
    "        image: wbassler/mobilenetv3lg-flask:v1.0\n",
    "        imagePullPolicy: Always\n",
    "        resources:\n",
    "          requests:\n",
    "            cpu: \"200m\"\n",
    "            memory: \"250Mi\"\n",
    "          limits:\n",
    "            cpu: \"200m\"\n",
    "            memory: \"250Mi\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taints\n",
    "```bash\n",
    "kubectl taint nodes node02 role=pytorch:NoSchedule\n",
    "```\n",
    "\n",
    "```bash\n",
    "kubectl describe node node02 | grep Taints\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tolerations\n",
    "```bash\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: ml-model-deployment\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: ml-model\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: ml-model\n",
    "    spec:\n",
    "      tolerations:\n",
    "      - key: \"key\"\n",
    "        operator: \"Equal\"\n",
    "        value: \"value\"\n",
    "        effect: \"NoSchedule\"\n",
    "      nodeSelector:\n",
    "        kubernetes.io/hostname: \"node02\"\n",
    "      containers:\n",
    "      - name: ml-model-container\n",
    "        image: wbassler/mobilenetv3lg-flask:v1.0\n",
    "        imagePullPolicy: Always\n",
    "        resources:\n",
    "          requests:\n",
    "            cpu: \"200m\"\n",
    "            memory: \"250Mi\"\n",
    "          limits:\n",
    "            cpu: \"200m\"\n",
    "            memory: \"250Mi\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning GPUs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
